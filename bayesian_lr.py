# import pymc as pm
import numpy as np
import pandas as pd
import arviz as az
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler

# ðŸ”¹ NumPy ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®šï¼ˆPyMC ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ã«ã‚‚å½±éŸ¿ï¼‰
np.random.seed(42)

# å„èªçŸ¥æ©Ÿèƒ½ã‚¹ã‚³ã‚¢ã§ãƒ™ã‚¤ã‚ºç·šå½¢å›žå¸°åˆ†æž
for col in df.columns[6:]:
    print('')
    print(f'============================================{col}============================================')

    # nanã‚’é™¤åŽ»ï¼ˆdf ã‚’æ›´æ–°ï¼‰
    df_removed = df[~df[col].isna()].copy()
    
    # èª¬æ˜Žå¤‰æ•°
    X = df_removed[['æ¤œæŸ»æ™‚ã®å¹´é½¢', 'æ€§åˆ¥', 'æ•™è‚²æ­´', 'group']].copy()
    
    # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆgroup: DPA = 0, naPPA = 1ï¼‰
    X['group'] = (df_removed['group'] == 'nappa').astype(int)  # naPPA = 1, DPA = 0
    X = pd.get_dummies(X, columns=['æ€§åˆ¥'], drop_first=True, dtype=int).copy()
    
    # ç›®çš„å¤‰æ•°ï¼ˆèªçŸ¥æ©Ÿèƒ½ã‚¹ã‚³ã‚¢ï¼‰
    y = df_removed[col].values  # é€£ç¶šå¤‰æ•°ã¨ã—ã¦æ‰±ã†

    # æ¨™æº–åŒ–ï¼ˆé€£ç¶šå¤‰æ•°ã®ã¿ï¼‰
    scaler = StandardScaler()
    num_cols = ['æ¤œæŸ»æ™‚ã®å¹´é½¢', 'æ•™è‚²æ­´']
    X[num_cols] = scaler.fit_transform(X[num_cols])

    # PyMCã«ã‚ˆã‚‹ãƒ™ã‚¤ã‚ºç·šå½¢å›žå¸°
    with pm.Model() as model:
        beta_age = pm.Normal("beta_age", mu=0, sigma=5)
        beta_edu = pm.Normal("beta_edu", mu=0, sigma=5)
        beta_gender = pm.Normal("beta_gender", mu=0, sigma=5)
        beta_group = pm.Normal("beta_group", mu=0, sigma=5)  # DPA vs naPPA ã®å½±éŸ¿
        intercept = pm.Normal("intercept", mu=0, sigma=10)

        # ç·šå½¢çµåˆ
        mu = (
            intercept
            + beta_age * X["æ¤œæŸ»æ™‚ã®å¹´é½¢"].values
            + beta_edu * X["æ•™è‚²æ­´"].values
            + beta_gender * X["æ€§åˆ¥_ç”·"].values
            + beta_group * X["group"].values  # DPA vs naPPA ã®å½±éŸ¿
        )

        sigma = pm.HalfNormal("sigma", sigma=5)  # ãƒŽã‚¤ã‚ºã®æ¨™æº–åå·®

        # ç·šå½¢å›žå¸°ã®å°¤åº¦ï¼ˆè¦³æ¸¬å€¤ y ã¯æ­£è¦åˆ†å¸ƒã«å¾“ã†ï¼‰
        y_obs = pm.Normal("y_obs", mu=mu, sigma=sigma, observed=y)

        # âœ… MCMCã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        trace = pm.sample(2000, tune=1000, return_inferencedata=True, idata_kwargs={"log_likelihood": True})

    # âœ… äº‹å¾Œåˆ†å¸ƒã®å¯è¦–åŒ–
    az.plot_trace(trace, figsize=(20, 12))
    plt.tight_layout()
    az.plot_posterior(trace, hdi_prob=0.95)
    plt.show()

    # äº‹å¾Œåˆ†å¸ƒã®è¦ç´„
    summary = az.summary(trace, stat_funcs={"median": np.median}, hdi_prob=0.95)
    print(summary)

    print('')

    # äº‹å¾Œç¢ºçŽ‡ã‚’è¨ˆç®—
    def compute_posterior_probabilities(trace, param):
        """äº‹å¾Œç¢ºçŽ‡ã‚’æ±‚ã‚ã‚‹"""
        samples = trace.posterior[param].values.flatten()
        prob_positive = (samples > 0).mean()
        prob_negative = (samples < 0).mean()
        return prob_positive, prob_negative

    print("\näº‹å¾Œç¢ºçŽ‡:")
    for param in ["beta_age", "beta_edu", "beta_gender", "beta_group"]:
        p_pos, p_neg = compute_posterior_probabilities(trace, param)
        print(f"{param}: P(Î² > 0) = {p_pos:.3f}, P(Î² < 0) = {p_neg:.3f}")
